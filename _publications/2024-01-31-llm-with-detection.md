---
title: "Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study"
collection: publications
permalink: /publication/2024-01-31-llm-with-detection
date: 2024-01-31
venue: 'arXiv'
citation: 'Jiao, Qirui, Daoyuan Chen, <ins>Yilun Huang</ins>, Yaliang Li, and Ying Shen. "Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study." arXiv preprint arXiv:2401.17981 (2024).'
paperurl: 'https://arxiv.org/abs/2401.17981'
# code: 'https://github.com/alibaba/data-juicer'
---

<strong>Abstraction</strong>: Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a notable advancement in multimodal understanding. We release our codes to facilitate further exploration into the fine-grained multimodal dialogue capabilities of MLLMs.

- Download paper [here](https://arxiv.org/abs/2401.17981)
<!-- - Code is available [here](https://github.com/alibaba/data-juicer) -->
